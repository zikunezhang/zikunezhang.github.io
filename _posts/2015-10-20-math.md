---
layout: post
title: A Modified Poincaré Inequality in Discrete Settings
date: 2025-10-20
description: An inequality implied by modified log-Sobolev inequality
tags: formatting math
categories: 
related_posts: false
---

<!-- This theme supports rendering beautiful math in inline and display modes using [MathJax 3](https://www.mathjax.org/) engine. You just need to surround your math expression with `$$`, like `$$ E = mc^2 $$`. If you leave it inside a paragraph, it will produce an inline expression, just like $$ E = mc^2 $$.

To use display mode, again surround your expression with `$$` and place it as a separate paragraph. Here is an example:

$$
\sum_{k=1}^\infty |\langle x, e_k \rangle|^2 \leq \|x\|^2
$$

You can also use `\begin{equation}...\end{equation}` instead of `$$` for display mode math.
MathJax will automatically number equations:

\begin{equation}
\label{eq:cauchy-schwarz}
\left( \sum_{k=1}^n a_k b_k \right)^2 \leq \left( \sum_{k=1}^n a_k^2 \right) \left( \sum_{k=1}^n b_k^2 \right)
\end{equation}

and by adding `\label{...}` inside the equation environment, we can now refer to the equation using `\eqref`.

Note that MathJax 3 is [a major re-write of MathJax](https://docs.mathjax.org/en/latest/upgrading/whats-new-3.0.html) that brought a significant improvement to the loading and rendering speed, which is now [on par with KaTeX](http://www.intmath.com/cg5/katex-mathjax-comparison.php). -->


This blog introduces the _modified Poincaré inequality_. Notably, analogous to the fact that log-Sobolev inequality implies Poincaré inequality, modified log-Sobolev inequality implies modified Poincaré inequality.

We first introduce some basic concepts. Let $\rho$ and $\mu$ be two distributions on $$[n]=\{ 1,\cdots,n \}$$ denoted by their probability mass functions. Assume that $\rho$ and $\mu$ have full support on $[n]$.

**Definition 1 ($\mu$-variance).** Let $f:[n]\to \mathbb{R}$ be a function and $\mu$ be a probability distribution on $[n]$. The variance of $f$ with respect to $\mu$ is defined as

$$
\operatorname{Var}_\mu[f]
= \mathbb{E}_\mu[(f-\mathbb{E}_\mu[f])^2]
= \mathbb{E}_\mu[f^2]-(\mathbb{E}_\mu[f])^2,
$$

where
$$
\mathbb{E}_\mu[f]=\sum_{x=1}^n \mu(x) f(x).
$$

**Definition 2 ($\mu$-covariance).** Let $\mu$ be a probability distribution on $[n]$. The covariance of two functions $f,g:[n]\to\mathbb{R}$ is defined by

$$
\operatorname{Cov}_\mu[f,g]=\mathbb{E}_\mu[(f-\mathbb{E}_\mu[f])(g-\mathbb{E}_\mu[g])]=\mathbb{E}_\mu[fg]-\mathbb{E}_\mu[f]\mathbb{E}_\mu[g].
$$

By Cauchy-Schwarz inequality, we have

$$
\operatorname{Cov}^2_\mu [f,g]\leq \operatorname{Var}_\mu[f] \operatorname{Var}_\mu[g].
$$

**Definition 3 ($\mu$-Entropy).** Let $f:[n]\to \mathbb{R}$ be a function and $\mu$ be a probability distribution on $[n]$. Define

$$
\operatorname{Ent}_\mu[f]=\mathbb{E}_\mu[f\log f]-\mathbb{E}_\mu[f]\log(\mathbb{E}_\mu[f]).
$$

It can be easily verified that $$\operatorname{Ent}_\mu[\frac{\rho}{\mu}]=D_\mathrm{KL}(\rho\Vert \mu)$$ for a probability distribution $\rho$ on $[n]$.

**Definition 4 (Dirichlet Form, see [Bobkov & Tetali (2006)](https://link.springer.com/article/10.1007/s10959-006-0016-3)).**
Let $([n],\mu)$ be a finite probability space, and let $P:[n]\times [n]\to [0,+\infty)$ be a non-negative function, called a kernel in the sequel. For all functions $f,g$ on $[n]$, one may define the associated Dirichlet form by

$$
\mathcal{E}_\mu(f,g)=\frac{1}{2} \int \sum_{y=1}^n (f(x)-f(y))(g(x)-g(y))P(x,y) \, \mathrm{d}\mu(x)=\frac{1}{2} \sum_{x,y=1}^n (f(x)-f(y))(g(x)-g(y))P(x,y)\mu(x).
$$

We further define corresponding gradient operator $$\nabla f(x)=\{\frac{1}{\sqrt{2}}(f(x)-f(y))\sqrt{P(x,y)}\}_{y\in [n]}$$, so we can write out the gradient formula as follows:

$$
\mathcal{E}_\mu(f,g)=\int \langle \nabla f(x),\nabla g(x)\rangle\, \mathrm{d}\mu(x).
$$


**Definition 5 (Log-Sobolev Inequality).**
Let $\mu$ be a probability distribution on $[n]$. We say $\mu$ satisfies the log-Sobolev inequality (LSI) with a constant $\alpha$ if for all function $f:[n]\to \mathbb{R}$ with $\operatorname{Ent}_\mu[f^2]<\infty$,

\begin{equation}
\label{def:LSI}
\frac{\alpha}{2}\operatorname{Ent}\_\mu[f^2]\leq \mathcal{E}_\mu(f,f).
\end{equation}

**Definition 6 (Modified Log-Sobolev Inequality, see [Bobkov & Tetali (2006)](https://link.springer.com/article/10.1007/s10959-006-0016-3)).**
Let $\mu$ be a probability distribution on $[n]$. We say $\mu$ satisfies the modified log-Sobolev inequality (MLSI) with a constant $\alpha$ if for all function $$f:[n]\to \mathbb{R}_{+}$$ with $$\operatorname{Ent}_\mu[f]<\infty$$,

\begin{equation}
\label{def:MLSI1}
2\alpha \operatorname{Ent}\_\mu[f]\leq \mathcal{E}_\mu(f,\log f).
\end{equation}

In general the LSI is strictly stronger than MLSI, i.e., LSI implies inequality MLSI (see, e.g., [Chewi (2023)](https://chewisinho.github.io/main.pdf)). As he noted, in many cases, LSI is too strong in that it does not hold with a good constant $\alpha$; hence, MLSI is often the more appropriate inequality for the discrete setting.

**Definition 7 (Poincaré Inequality)**
Let $\mu$ be a probability distribution on $[n]$. We say $\mu$ satisfies Poincaré inequality (PI) with a constant $\alpha$ if for all functions $f: [n] \to \mathbb{R}$, it holds that

\begin{equation}
\label{def:PI}
\alpha \cdot \operatorname{Var}\_\mu [f] \leq \mathcal{E}_\mu(f,f).
\end{equation}


We know that LSI implies PI with the same constant (see, e.g., [Ledoux (2001)](https://www.numdam.org/item/SPS_2001__35__167_0.pdf)) and MLSI implies PI (see, e.g., [Bobkov & Tetali (2006)](https://link.springer.com/article/10.1007/s10959-006-0016-3)).


We next introduce the modified Poincaré inequality:

**Definition 8 (Modified Poincaré Inequality)**
Let $\mu$ be a distribution on $[n]$. We say that $\mu$ satisfies modified Poincaré inequality (MPI) with constant $\alpha$ if for all function $f:[n]\to \mathbb{R}_{+}$, the following inequality holds:

\begin{equation}
\label{MPI}
4\alpha \cdot \operatorname{Var}\_\mu[\sqrt{f}]\leq \mathcal{E}_\mu(f,\log f).
\end{equation}

We can prove that MLSI implies MPI with the same constant. Moreover, PI implies MPI because of the inequality $4(\sqrt{a}-\sqrt{b})^2\leq (\log a-\log b)(a-b)$ for tow positive $a$ and $b$. Thus as discussed above, we have the relations between these inequalities with the same constant:

$$
\begin{array}{ccc}
    \mathrm{LSI} \ \eqref{def:LSI} & \Longrightarrow & \mathrm{PI} \ \eqref{def:PI} \\
    \Downarrow & \nearrow & \Downarrow \\
    \mathrm{MLSI} \ \eqref{def:MLSI1} & \Longrightarrow & \mathrm{MPI} \ \eqref{MPI}.
\end{array}
$$

Finally, we prove that MLSI implies MPI with the same constant.

_Proof._ If $\mu$ satisfies MLSI with constant $\alpha$, we have that for all function $f:[n]\to \mathbb{R}_{+}$, it holds that

\begin{equation}\label{lemmaeq:MLSI}
2\alpha\cdot \operatorname{Ent}\_\mu[f]\leq \mathcal{E}_\mu(f,\log f).
\end{equation}

Note that for a small $\epsilon>0$,

$$
\begin{align}
&\operatorname{Ent}_\mu [1+\epsilon \sqrt{f}]\notag\\
={}&\mathbb{E}_\mu [(1+\epsilon \sqrt{f})\log(1+\epsilon\sqrt{f})]-\mathbb{E}_\mu[1+\epsilon \sqrt{f}]\log\mathbb{E}_\mu[1+\epsilon\sqrt{f}]\notag\\
={}&\mathbb{E}_\mu [(1+\epsilon \sqrt{f})(\epsilon\sqrt{f}-\frac{\epsilon^2}{2}f+O(\epsilon^3)]-(1+\epsilon\mathbb{E}_\mu[\sqrt{f}])(\epsilon\mathbb{E}_\mu[\sqrt{f}]-\frac{\epsilon^2}{2}\mathbb{E}_\mu[\sqrt{f}]^2+O(\epsilon^3))\notag\\
={}&\frac{\epsilon^2}{2}\operatorname{Var}_\mu [\sqrt{f}]+O(\epsilon^3)\label{ent},
\end{align}
$$

and

$$
\begin{align}
&\mathcal{E}_\mu (1+\epsilon\sqrt{f},\log (1+\epsilon\sqrt{f}))\notag\\
={}&\frac{1}{2}\sum_{x,y=1}^n \mu(x)P(x,y)(\epsilon(\sqrt{f(x)}-\sqrt{f(y)}))(\log(1+\epsilon\sqrt{f(x)})-\log(1+\epsilon\sqrt{f(y)}))\notag\\
={}&\frac{1}{2}\sum_{x,y=1}^n \mu(x)P(x,y)\epsilon(\sqrt{f(x)}-\sqrt{f(y)})(\epsilon\xi_\epsilon(x,y)(\sqrt{f(x)}-\sqrt{f(y)}))\notag\\
={}&\epsilon^2\cdot\frac{1}{2}\sum_{x,y=1}^n \mu(x)P(x,y)(\sqrt{f(x)}-\sqrt{f(y)})^2\xi_\epsilon(x,y)\notag\\
\leq{}&\frac{\epsilon^2}{4}\frac{1}{2}\sum_{x,y=1}^n \mu(x)P(x,y)(f(x)-f(y))(\log f(x)-\log f(y))\xi_\epsilon(x,y),\label{dirichlet}
\end{align}
$$

where $\xi_\epsilon(x,y)$ is between  $1+\epsilon\sqrt{f(x)}$ and $1+\epsilon\sqrt{f(y)}$, and when $\epsilon\to 0$, $\xi_\epsilon(x,y)\to 1$. The inequality in \eqref{dirichlet} holds since $4(\sqrt{a}-\sqrt{b})^2=(\int_a^b t^{-1/2}\mathrm{d}t)^2\leq (\int_a^b 1^2 \mathrm{d}t)(\int_a^b t^{-1}\mathrm{d}t)=(a-b)(\log a-\log b)$ for all $a,b>0$, where the inequality is by Cauchy-Schwarz inequality. Thus, by applying \eqref{lemmaeq:MLSI} to $1+\epsilon \sqrt{f}$ where $\epsilon>0$ is small, combining with \eqref{ent} and \eqref{dirichlet} and letting $\epsilon$ tend to 0, we obtain that
$$
4\alpha \cdot \operatorname{Var}_\mu[\sqrt{f}]\leq \mathcal{E}_\mu(f,\log f).
$$
We conclude the proof.$\square$