<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://zikunezhang.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://zikunezhang.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2026-01-01T23:16:40+00:00</updated><id>https://zikunezhang.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">A Modified Poincaré Inequality in Discrete Settings</title><link href="https://zikunezhang.github.io/blog/2025/math/" rel="alternate" type="text/html" title="A Modified Poincaré Inequality in Discrete Settings"/><published>2025-10-20T00:00:00+00:00</published><updated>2025-10-20T00:00:00+00:00</updated><id>https://zikunezhang.github.io/blog/2025/math</id><content type="html" xml:base="https://zikunezhang.github.io/blog/2025/math/"><![CDATA[ <p>In this blog, we introduce modified Poincaré inequality. Notably, analogous to the fact that log-Sobolev inequality implies Poincaré inequality, modified log-Sobolev inequality implies modified Poincaré inequality.</p> <p>We first introduce some basic concepts. Let $\rho$ and $\mu$ be two distributions on \([n]=\{ 1,\cdots,n \}\) denoted by their probability mass functions. Assume that $\rho$ and $\mu$ have full support on $[n]$.</p> <p><strong>Definition 1 ($\mu$-variance).</strong> Let $f:[n]\to \mathbb{R}$ be a function and $\mu$ be a probability distribution on $[n]$. The variance of $f$ with respect to $\mu$ is defined as</p> \[\operatorname{Var}_\mu[f] = \mathbb{E}_\mu[(f-\mathbb{E}_\mu[f])^2] = \mathbb{E}_\mu[f^2]-(\mathbb{E}_\mu[f])^2,\] <p>where \(\mathbb{E}_\mu[f]=\sum_{x=1}^n \mu(x) f(x).\)</p> <p><strong>Definition 2 ($\mu$-covariance).</strong> Let $\mu$ be a probability distribution on $[n]$. The covariance of two functions $f,g:[n]\to\mathbb{R}$ is defined by</p> \[\operatorname{Cov}_\mu[f,g]=\mathbb{E}_\mu[(f-\mathbb{E}_\mu[f])(g-\mathbb{E}_\mu[g])]=\mathbb{E}_\mu[fg]-\mathbb{E}_\mu[f]\mathbb{E}_\mu[g].\] <p>By Cauchy-Schwarz inequality, we have</p> \[\operatorname{Cov}^2_\mu [f,g]\leq \operatorname{Var}_\mu[f] \operatorname{Var}_\mu[g].\] <p><strong>Definition 3 ($\mu$-Entropy).</strong> Let $f:[n]\to \mathbb{R}$ be a function and $\mu$ be a probability distribution on $[n]$. Define</p> \[\operatorname{Ent}_\mu[f]=\mathbb{E}_\mu[f\log f]-\mathbb{E}_\mu[f]\log(\mathbb{E}_\mu[f]).\] <p>It can be easily verified that \(\operatorname{Ent}_\mu[\frac{\rho}{\mu}]=D_\mathrm{KL}(\rho\Vert \mu)\) for a probability distribution $\rho$ on $[n]$.</p> <p><strong>Definition 4 (Dirichlet Form, see <a href="https://link.springer.com/article/10.1007/s10959-006-0016-3">Bobkov &amp; Tetali (2006)</a>).</strong> Let $([n],\mu)$ be a finite probability space, and let $P:[n]\times [n]\to [0,+\infty)$ be a non-negative function, called a kernel in the sequel. For all functions $f,g$ on $[n]$, one may define the associated Dirichlet form by</p> \[\mathcal{E}_\mu(f,g)=\frac{1}{2} \int \sum_{y=1}^n (f(x)-f(y))(g(x)-g(y))P(x,y) \, \mathrm{d}\mu(x)=\frac{1}{2} \sum_{x,y=1}^n (f(x)-f(y))(g(x)-g(y))P(x,y)\mu(x).\] <p>We further define corresponding gradient operator \(\nabla f(x)=\{\frac{1}{\sqrt{2}}(f(x)-f(y))\sqrt{P(x,y)}\}_{y\in [n]}\), so we can write out the gradient formula as follows:</p> \[\mathcal{E}_\mu(f,g)=\int \langle \nabla f(x),\nabla g(x)\rangle\, \mathrm{d}\mu(x).\] <p><strong>Definition 5 (Log-Sobolev Inequality).</strong> Let $\mu$ be a probability distribution on $[n]$. We say $\mu$ satisfies the log-Sobolev inequality (LSI) with a constant $\alpha$ if for all function $f:[n]\to \mathbb{R}$ with $\operatorname{Ent}_\mu[f^2]&lt;\infty$,</p> <p>\begin{equation} \label{def:LSI} \frac{\alpha}{2}\operatorname{Ent}_\mu[f^2]\leq \mathcal{E}_\mu(f,f). \end{equation}</p> <p><strong>Definition 6 (Modified Log-Sobolev Inequality, see <a href="https://link.springer.com/article/10.1007/s10959-006-0016-3">Bobkov &amp; Tetali (2006)</a>).</strong> Let $\mu$ be a probability distribution on $[n]$. We say $\mu$ satisfies the modified log-Sobolev inequality (MLSI) with a constant $\alpha$ if for all function \(f:[n]\to \mathbb{R}_{+}\) with \(\operatorname{Ent}_\mu[f]&lt;\infty\),</p> <p>\begin{equation} \label{def:MLSI1} 2\alpha \operatorname{Ent}_\mu[f]\leq \mathcal{E}_\mu(f,\log f). \end{equation}</p> <p>In general the LSI is strictly stronger than MLSI, i.e., LSI implies inequality MLSI (see, e.g., <a href="https://chewisinho.github.io/main.pdf">chewi (2023)</a>). As he noted, in many cases, LSI is too strong in that it does not hold with a good constant $\alpha$; hence, MLSI is often the more appropriate inequality for the discrete setting.</p> <p><strong>Definition 7 (Poincaré Inequality)</strong> Let $\mu$ be a probability distribution on $[n]$. We say $\mu$ satisfies Poincaré inequality (PI) with a constant $\alpha$ if for all functions $f: [n] \to \mathbb{R}$, it holds that</p> <p>\begin{equation} \label{def:PI} \alpha \cdot \operatorname{Var}_\mu [f] \leq \mathcal{E}_\mu(f,f). \end{equation}</p> <p>We know that LSI implies PI with the same constant (see, e.g., <a href="https://www.numdam.org/item/SPS_2001__35__167_0.pdf">Ledoux (2001)</a>) and MLSI implies PI (see, e.g., <a href="https://link.springer.com/article/10.1007/s10959-006-0016-3">Bobkov &amp; Tetali (2006)</a>).</p> <p>We next introduce modified Poincaré inequality:</p> <p><strong>Definition 8 (Modified Poincaré Inequality)</strong> Let $\mu$ be a distribution on $[n]$. We say that $\mu$ satisfies modified Poincaré inequality (MPI) with constant $\alpha$ if for all function $f:[n]\to \mathbb{R}_{+}$, the following inequality holds:</p> <p>\begin{equation} \label{MPI} 4\alpha \cdot \operatorname{Var}_\mu[\sqrt{f}]\leq \mathcal{E}_\mu(f,\log f). \end{equation}</p> <p>We can prove that MLSI implies MPI with the same constant. Moreover, PI implies MPI because of the inequality $4(\sqrt{a}-\sqrt{b})^2\leq (\log a-\log b)(a-b)$ for tow positive $a$ and $b$. Thus as discussed above, we have the relations between these inequalities with the same constant:</p> \[\begin{array}{ccc} \mathrm{LSI} \ \eqref{def:LSI} &amp; \Longrightarrow &amp; \mathrm{PI} \ \eqref{def:PI} \\ \Downarrow &amp; \nearrow &amp; \Downarrow \\ \mathrm{MLSI} \ \eqref{def:MLSI1} &amp; \Longrightarrow &amp; \mathrm{MPI} \ \eqref{MPI}. \end{array}\] <p>Finally, we prove that MLSI implies MPI with the same constant.</p> <p><em>Proof.</em> If $\mu$ satisfies MLSI with constant $\alpha$, we have that for all function $f:[n]\to \mathbb{R}_{+}$, it holds that</p> <p>\begin{equation}\label{lemmaeq:MLSI} 2\alpha\cdot \operatorname{Ent}_\mu[f]\leq \mathcal{E}_\mu(f,\log f). \end{equation}</p> <p>Note that for a small $\epsilon&gt;0$,</p> \[\begin{align} &amp;\operatorname{Ent}_\mu [1+\epsilon \sqrt{f}]\notag\\ ={}&amp;\mathbb{E}_\mu [(1+\epsilon \sqrt{f})\log(1+\epsilon\sqrt{f})]-\mathbb{E}_\mu[1+\epsilon \sqrt{f}]\log\mathbb{E}_\mu[1+\epsilon\sqrt{f}]\notag\\ ={}&amp;\mathbb{E}_\mu [(1+\epsilon \sqrt{f})(\epsilon\sqrt{f}-\frac{\epsilon^2}{2}f+O(\epsilon^3)]-(1+\epsilon\mathbb{E}_\mu[\sqrt{f}])(\epsilon\mathbb{E}_\mu[\sqrt{f}]-\frac{\epsilon^2}{2}\mathbb{E}_\mu[\sqrt{f}]^2+O(\epsilon^3))\notag\\ ={}&amp;\frac{\epsilon^2}{2}\operatorname{Var}_\mu [\sqrt{f}]+O(\epsilon^3)\label{ent}, \end{align}\] <p>and</p> \[\begin{align} &amp;\mathcal{E}_\mu (1+\epsilon\sqrt{f},\log (1+\epsilon\sqrt{f}))\notag\\ ={}&amp;\frac{1}{2}\sum_{x,y=1}^n \mu(x)P(x,y)(\epsilon(\sqrt{f(x)}-\sqrt{f(y)}))(\log(1+\epsilon\sqrt{f(x)})-\log(1+\epsilon\sqrt{f(y)}))\notag\\ ={}&amp;\frac{1}{2}\sum_{x,y=1}^n \mu(x)P(x,y)\epsilon(\sqrt{f(x)}-\sqrt{f(y)})(\epsilon\xi_\epsilon(x,y)(\sqrt{f(x)}-\sqrt{f(y)}))\notag\\ ={}&amp;\epsilon^2\cdot\frac{1}{2}\sum_{x,y=1}^n \mu(x)P(x,y)(\sqrt{f(x)}-\sqrt{f(y)})^2\xi_\epsilon(x,y)\notag\\ \leq{}&amp;\frac{\epsilon^2}{4}\frac{1}{2}\sum_{x,y=1}^n \mu(x)P(x,y)(f(x)-f(y))(\log f(x)-\log f(y))\xi_\epsilon(x,y),\label{dirichlet} \end{align}\] <p>where $\xi_\epsilon(x,y)$ is between $1+\epsilon\sqrt{f(x)}$ and $1+\epsilon\sqrt{f(y)}$, and when $\epsilon\to 0$, $\xi_\epsilon(x,y)\to 1$. The inequality in \eqref{dirichlet} holds since $4(\sqrt{a}-\sqrt{b})^2=(\int_a^b t^{-1/2}\mathrm{d}t)^2\leq (\int_a^b 1^2 \mathrm{d}t)(\int_a^b t^{-1}\mathrm{d}t)=(a-b)(\log a-\log b)$ for all $a,b&gt;0$, where the inequality is by Cauchy-Schwarz inequality. Thus, by applying \eqref{lemmaeq:MLSI} to $1+\epsilon \sqrt{f}$ where $\epsilon&gt;0$ is small, combining with \eqref{ent} and \eqref{dirichlet} and letting $\epsilon$ tend to 0, we obtain that \(4\alpha \cdot \operatorname{Var}_\mu[\sqrt{f}]\leq \mathcal{E}_\mu(f,\log f).\) We conclude the proof.$\square$</p>]]></content><author><name></name></author><category term="formatting"/><category term="math"/><summary type="html"><![CDATA[An inequality implied by modified log-Sobolev inequality]]></summary></entry></feed>